"""Tests for vlora.model — VLoRAModel inference wrapper."""

import torch
import torch.nn as nn
import pytest

from vlora.io import LoRAWeights
from vlora.model import VLoRAModel
from vlora.subspace import SharedSubspace


def _make_base_model(layers, dim=64):
    """Create a simple base model with named linear layers matching adapter layer names."""
    modules = {}
    for layer_name in layers:
        # Convert dot-notation to nested modules
        modules[layer_name] = nn.Linear(dim, dim, bias=False)

    # Build a model from the modules using a ModuleDict-like approach
    # We need nested module structure to match layer names like "layer.0.q_proj"
    model = _NestedModel(modules)
    return model


class _NestedModel(nn.Module):
    """Simple model that registers linear layers at dot-separated paths."""

    def __init__(self, named_layers: dict[str, nn.Module]):
        super().__init__()
        for name, module in named_layers.items():
            # Register each as a flat module with dots replaced
            parts = name.split(".")
            parent = self
            for part in parts[:-1]:
                if not hasattr(parent, part):
                    child = nn.Module()
                    parent.add_module(part, child)
                parent = getattr(parent, part)
            parent.add_module(parts[-1], module)

    def forward(self, x):
        # Simple pass-through for testing — apply all linear layers sequentially
        for name, module in self.named_modules():
            if isinstance(module, nn.Linear):
                x = module(x)
        return x


def _make_adapters_and_model(n=3, layers=None, rank=4, dim=64):
    """Create adapters, subspace, and a matching base model."""
    if layers is None:
        layers = ["layer.0.q_proj", "layer.0.v_proj"]

    shared_a = {l: torch.randn(3, rank * dim) for l in layers}
    shared_b = {l: torch.randn(3, dim * rank) for l in layers}

    adapters = []
    for i in range(n):
        lora_a = {l: (torch.randn(3) @ shared_a[l]).reshape(rank, dim) for l in layers}
        lora_b = {l: (torch.randn(3) @ shared_b[l]).reshape(dim, rank) for l in layers}
        adapters.append(LoRAWeights(layer_names=layers, lora_a=lora_a, lora_b=lora_b, rank=rank))

    sub = SharedSubspace.from_adapters(adapters, num_components=2)
    base_model = _make_base_model(layers, dim=dim)
    return sub, base_model, layers


class TestVLoRAModel:
    def test_set_task(self):
        sub, base_model, _ = _make_adapters_and_model()
        model = VLoRAModel(base_model, sub)
        model.set_task("task_0")
        assert model.active_task == "task_0"

    def test_clear_task(self):
        sub, base_model, _ = _make_adapters_and_model()
        model = VLoRAModel(base_model, sub)
        model.set_task("task_0")
        model.clear_task()
        assert model.active_task is None

    def test_available_tasks(self):
        sub, base_model, _ = _make_adapters_and_model(n=3)
        model = VLoRAModel(base_model, sub)
        assert model.available_tasks == ["task_0", "task_1", "task_2"]

    def test_unknown_task_raises(self):
        sub, base_model, _ = _make_adapters_and_model()
        model = VLoRAModel(base_model, sub)
        with pytest.raises(KeyError, match="Unknown task"):
            model.set_task("nonexistent")

    def test_forward_without_task(self):
        sub, base_model, _ = _make_adapters_and_model(dim=64)
        model = VLoRAModel(base_model, sub)
        x = torch.randn(2, 64)
        out = model(x)
        assert out.shape[0] == 2

    def test_forward_with_task_changes_output(self):
        sub, base_model, _ = _make_adapters_and_model(dim=64)
        model = VLoRAModel(base_model, sub)

        x = torch.randn(2, 64)
        out_base = model(x).detach().clone()

        model.set_task("task_0")
        out_lora = model(x).detach().clone()

        # Output should differ when LoRA is applied
        assert not torch.allclose(out_base, out_lora, atol=1e-6)

    def test_switching_tasks_changes_output(self):
        sub, base_model, _ = _make_adapters_and_model(dim=64)
        model = VLoRAModel(base_model, sub)

        x = torch.randn(2, 64)

        model.set_task("task_0")
        out_0 = model(x).detach().clone()

        model.set_task("task_1")
        out_1 = model(x).detach().clone()

        # Different tasks should generally give different outputs
        # (not guaranteed but very likely with random adapters)
        assert not torch.allclose(out_0, out_1, atol=1e-6)

    def test_same_task_is_cached(self):
        sub, base_model, _ = _make_adapters_and_model()
        model = VLoRAModel(base_model, sub)
        model.set_task("task_0")
        cached = model._cached_deltas
        model.set_task("task_0")  # Same task — should not recompute
        assert model._cached_deltas is cached

    def test_reconstruct_state_dict(self):
        sub, base_model, layers = _make_adapters_and_model(dim=64)
        model = VLoRAModel(base_model, sub)
        deltas = model.reconstruct_state_dict("task_0")
        for l in layers:
            assert l in deltas
            assert deltas[l].shape == (64, 64)  # (out_features, in_features)
