{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLoRA Quickstart\n",
    "\n",
    "**Shared low-rank subspaces for efficient LoRA adapter management.**\n",
    "\n",
    "This notebook demonstrates vlora's core workflow: compress multiple LoRA adapters into a shared subspace, reconstruct on demand, merge adapters, and analyze similarity — all without needing a GPU.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tveseli/vlora/blob/main/examples/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q vlora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Synthetic Adapters\n",
    "\n",
    "In practice you'd load real PEFT adapters from disk or HuggingFace Hub. Here we create synthetic ones that share a low-rank structure (simulating adapters fine-tuned on related tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vlora import LoRAWeights, SharedSubspace\n",
    "\n",
    "# Simulate 8 adapters sharing a 3D subspace\n",
    "layers = [\"model.layers.0.self_attn.q_proj\", \"model.layers.0.self_attn.v_proj\"]\n",
    "rank, dim = 16, 512\n",
    "\n",
    "# Shared structure (unknown to vlora — it discovers this via SVD)\n",
    "shared_a = {l: torch.randn(3, rank * dim) for l in layers}\n",
    "shared_b = {l: torch.randn(3, dim * rank) for l in layers}\n",
    "\n",
    "adapters = []\n",
    "for i in range(8):\n",
    "    lora_a = {l: (torch.randn(3) @ shared_a[l] + torch.randn(rank * dim) * 0.01).reshape(rank, dim) for l in layers}\n",
    "    lora_b = {l: (torch.randn(3) @ shared_b[l] + torch.randn(dim * rank) * 0.01).reshape(dim, rank) for l in layers}\n",
    "    adapters.append(LoRAWeights(layer_names=layers, lora_a=lora_a, lora_b=lora_b, rank=rank))\n",
    "\n",
    "print(f\"Created {len(adapters)} adapters: rank={rank}, dim={dim}, layers={len(layers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Shared Subspace\n",
    "\n",
    "SVD discovers the shared basis across all adapters. Each adapter is then represented as a small loadings vector instead of full weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subspace = SharedSubspace.from_adapters(adapters, num_components=4)\n",
    "\n",
    "print(f\"Components (k): {subspace.num_components}\")\n",
    "print(f\"Tasks: {len(subspace.tasks)}\")\n",
    "print(f\"Layers: {len(subspace.layer_names)}\")\n",
    "\n",
    "# Compression stats\n",
    "stats = subspace.compression_stats()\n",
    "print(f\"\\nCompression ratio: {stats['compression_ratio']:.1f}×\")\n",
    "print(f\"Original params:   {stats['total_params_original']:,}\")\n",
    "print(f\"Compressed params:  {stats['total_params_compressed']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reconstruct & Measure Error\n",
    "\n",
    "Reconstruct any task back to full LoRA weights and measure how close we are to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlora import explained_variance_ratio\n",
    "\n",
    "# Reconstruction error\n",
    "for tid in [\"task_0\", \"task_3\", \"task_7\"]:\n",
    "    recon = subspace.reconstruct(tid)\n",
    "    idx = int(tid.split(\"_\")[1])\n",
    "    original = adapters[idx]\n",
    "    \n",
    "    errors = []\n",
    "    for l in layers:\n",
    "        err_a = (recon.lora_a[l] - original.lora_a[l]).norm() / original.lora_a[l].norm()\n",
    "        err_b = (recon.lora_b[l] - original.lora_b[l]).norm() / original.lora_b[l].norm()\n",
    "        errors.extend([err_a.item(), err_b.item()])\n",
    "    \n",
    "    print(f\"{tid}: mean_error={sum(errors)/len(errors):.4f}, max_error={max(errors):.4f}\")\n",
    "\n",
    "# Variance explained\n",
    "first_layer = subspace.layer_names[0]\n",
    "var_a = explained_variance_ratio(subspace.singular_values_a[first_layer])\n",
    "var_b = explained_variance_ratio(subspace.singular_values_b[first_layer])\n",
    "k = subspace.num_components\n",
    "print(f\"\\nVariance explained at k={k}: A={var_a[k-1]:.1%}, B={var_b[k-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Project & Absorb New Adapters\n",
    "\n",
    "Add new adapters without rebuilding from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new adapter\n",
    "new_a = {l: (torch.randn(3) @ shared_a[l]).reshape(rank, dim) for l in layers}\n",
    "new_b = {l: (torch.randn(3) @ shared_b[l]).reshape(dim, rank) for l in layers}\n",
    "new_adapter = LoRAWeights(layer_names=layers, lora_a=new_a, lora_b=new_b, rank=rank)\n",
    "\n",
    "# Project (fast, no SVD recompute)\n",
    "proj = subspace.project(new_adapter, \"new_fast\")\n",
    "subspace.add_task(proj)\n",
    "print(f\"After project: {len(subspace.tasks)} tasks\")\n",
    "\n",
    "# Absorb incremental (updates basis with new directions)\n",
    "another = LoRAWeights(\n",
    "    layer_names=layers,\n",
    "    lora_a={l: (torch.randn(3) @ shared_a[l]).reshape(rank, dim) for l in layers},\n",
    "    lora_b={l: (torch.randn(3) @ shared_b[l]).reshape(dim, rank) for l in layers},\n",
    "    rank=rank,\n",
    ")\n",
    "subspace.absorb_incremental(another, \"new_absorbed\")\n",
    "print(f\"After absorb:  {len(subspace.tasks)} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adapter Merging\n",
    "\n",
    "Merge multiple adapters into one using task arithmetic, TIES, or DARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlora import task_arithmetic, ties_merge, dare_merge\n",
    "\n",
    "subset = adapters[:3]\n",
    "\n",
    "# Task arithmetic (weighted average)\n",
    "merged_avg = task_arithmetic(subset, weights=[0.5, 0.3, 0.2])\n",
    "print(f\"Task arithmetic: rank={merged_avg.rank}, layers={len(merged_avg.layer_names)}\")\n",
    "\n",
    "# TIES (trim + elect sign + merge)\n",
    "merged_ties = ties_merge(subset, density=0.5)\n",
    "print(f\"TIES merge:      rank={merged_ties.rank}, layers={len(merged_ties.layer_names)}\")\n",
    "\n",
    "# DARE (drop and rescale)\n",
    "merged_dare = dare_merge(subset, drop_rate=0.5, seed=42)\n",
    "print(f\"DARE merge:      rank={merged_dare.rank}, layers={len(merged_dare.layer_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adapter Analysis\n",
    "\n",
    "Analyze similarity between adapters and find redundant clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlora import compute_similarity_matrix, find_clusters\n",
    "\n",
    "sim = compute_similarity_matrix(adapters)\n",
    "print(\"Similarity matrix (first 4):\")\n",
    "for i in range(4):\n",
    "    row = \" \".join(f\"{sim[i,j]:.3f}\" for j in range(4))\n",
    "    print(f\"  task_{i}: {row}\")\n",
    "\n",
    "clusters = find_clusters(sim, threshold=0.9)\n",
    "print(f\"\\nClusters at threshold=0.9: {len(clusters)}\")\n",
    "for i, c in enumerate(clusters):\n",
    "    print(f\"  Cluster {i}: {['task_' + str(j) for j in c]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save & Load\n",
    "\n",
    "Subspaces serialize to safetensors + JSON metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, os\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    subspace.save(os.path.join(tmp, \"subspace\"))\n",
    "    files = os.listdir(os.path.join(tmp, \"subspace\"))\n",
    "    print(f\"Saved files: {sorted(files)}\")\n",
    "    \n",
    "    loaded = SharedSubspace.load(os.path.join(tmp, \"subspace\"))\n",
    "    print(f\"Loaded: k={loaded.num_components}, tasks={len(loaded.tasks)}, layers={len(loaded.layer_names)}\")\n",
    "    \n",
    "    # Verify roundtrip\n",
    "    recon_before = subspace.reconstruct(\"task_0\")\n",
    "    recon_after = loaded.reconstruct(\"task_0\")\n",
    "    err = (recon_before.lora_a[layers[0]] - recon_after.lora_a[layers[0]]).norm().item()\n",
    "    print(f\"Roundtrip error: {err:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Real adapters**: `pip install vlora[hub]` and use `load_adapter_from_hub(\"repo/name\")`\n",
    "- **CLI**: `vlora compress`, `vlora merge`, `vlora analyze` from the command line\n",
    "- **Training**: Use `SubspaceTrainer` or `VLoRACallback` with HF Trainer\n",
    "- **Inference**: Wrap your model with `VLoRAModel` for instant adapter switching\n",
    "- **Docs**: [github.com/tveseli/vlora](https://github.com/tveseli/vlora)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
